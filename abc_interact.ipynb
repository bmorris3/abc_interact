{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Demystifying Approximate Bayesian Computation\n",
    "\n",
    "## Brett Morris\n",
    "\n",
    "### In this tutorial\n",
    "\n",
    "We will write our own rejection sampling algorithm to approximate the posterior distributions for some fitting parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import anderson_ksamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's generate a series of observations $y_\\mathrm{obs}$, taken at times $x$. The observations will be drawn from one of two Gaussian distributions with a fixed standard deviation, separated by $3\\sigma$ from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "true_std = 1\n",
    "true_mean1 = np.pi\n",
    "true_mean2 = 3 * np.pi\n",
    "\n",
    "x = np.linspace(0, 1, 250)\n",
    "\n",
    "y_obs = np.concatenate([true_mean1 + true_std * np.random.randn(len(x)), \n",
    "                        true_mean2 + true_std * np.random.randn(len(x))])\n",
    "\n",
    "plt.hist(y_obs, bins=50)\n",
    "plt.xlabel('$y_\\mathrm{obs}$', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does one fit for the means and standard deviations of the bimodal distribution? One way is to use [Gaussian mixture models](https://dfm.io/posts/mixture-models/), but we're going to take a different approach. \n",
    "\n",
    "## Approximate Bayesian Computation\n",
    "\n",
    "For this particular dataset, it's easy to construct a model $\\mathcal{M}$ which reproduces the observations $y_\\mathcal{obs}$ – the model is simply the concatenation of two normal distributions $\\mathcal{M} \\sim [\\mathcal{N}(\\mu_1, \\sigma), \\mathcal{N}(\\mu_2, \\sigma))]$. One way to *approximate* the posterior distributions of $\\theta = \\{\\mu_1, \\mu_2, \\sigma\\}$ would be to propose new parameters $\\theta^*$, and only keep a running list of the parameter combinations which produce a simulated dataset $y_\\mathrm{sim}$ which very closely reproduces the observations $y_\\mathrm{obs}$. \n",
    "\n",
    "### Summary statistic: the Anderson-Darling statistic\n",
    "\n",
    "In practice, this requires a *summary statistic*, which measures the \"distance\" between the simulated dataset $y_\\mathrm{sim}$ and the observations $y_\\mathrm{obs}$. In this example we need a metric which measures the probability that two randomly-drawn samples $y$ are drawn from the same distribution. One such metric is the [Anderson-Darling statistic](https://en.wikipedia.org/wiki/Anderson–Darling_test), which approaches a minimum near $A^2=-1.3$ for two sets $y$ that are drawn from indistinguishable distributions, and grows to $A^2 > 10^5$ for easily distinguishable distributions.\n",
    "\n",
    "We can see how the Anderson-Darling statistic behaves in this simple example below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "# Generaet a bimodal distribution\n",
    "a = np.concatenate([np.random.randn(n_samples), \n",
    "                    3 + np.random.randn(n_samples//2)])\n",
    "\n",
    "# Plot the bimodal distribution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3))\n",
    "ax[0].hist(a, color='silver', range=[-4, 11], bins=50,\n",
    "           lw=2, histtype='stepfilled')\n",
    "\n",
    "# For a set of bimodal distributions with varing means: \n",
    "for mean in [0, 1.2, 5]: \n",
    "    # Generate a new bimodal distribution\n",
    "    c = mean + np.concatenate([np.random.randn(n_samples), \n",
    "                               3 + np.random.randn(n_samples//2)])\n",
    "    \n",
    "    # Measure, plot the Anderson-darling statistic\n",
    "    a2 = anderson_ksamp([a, c]).statistic\n",
    "    ax[0].hist(c, histtype='step', range=[-4, 11], \n",
    "               bins=50, lw=2)\n",
    "\n",
    "    ax[1].plot(mean, a2, 'o')\n",
    "\n",
    "ax[0].set(xlabel='Samples', ylabel='Frequency')\n",
    "ax[1].set(xlabel='Mean', ylabel='$A^2$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we have a set of observations $y_\\mathrm{obs}$ (left, gray) which we're comparing to the set of simulated observations $y_\\mathrm{sim}$ (left, colors). The Anderson-Darling statistic $A^2$ is plotted for each pair of the observations and the simulations (right). You can see that the minimum of $A^2$ is near -1, and it grows very large when $y_\\mathrm{obs}$ and $y_\\mathrm{sim}$ distributions are significantly different.\n",
    "\n",
    "### Rejection sampler\n",
    "\n",
    "We're now have the ingredients we need to create a *rejection sampler*, which will follow this algorithm: \n",
    "\n",
    "  1. Perturb initial/previous parameters $\\theta$ by a small amount to generate new trial parameters $\\theta^*$\n",
    "  \n",
    "  2. If the trial parameters $\\theta^*$ are drawn from within the prior, continue, else return to (1)\n",
    "  \n",
    "  3. Generate an example dataset $y_\\mathrm{sim}$ using your model $\\mathcal{M}$ \n",
    "  \n",
    "  4. Compute _distance_ between the simulated and observed datasets $\\rho(y_\\mathrm{obs}, y_\\mathrm{sim})$\n",
    "  \n",
    "  5. For some tolerance $h$, accept the step ($\\theta^* = \\theta$) if distance $\\rho(y_\\mathrm{obs}, y_\\mathrm{sim}) \\leq h$\n",
    "  \n",
    "  6. Return to step (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprior(theta): \n",
    "    \"\"\"\n",
    "    Define a prior probability, which simply requires \n",
    "    that -10 < mu_1, mu_2 < 20 and 0 < sigma < 10\n",
    "    \"\"\"\n",
    "    mean1, mean2, std = theta\n",
    "    \n",
    "    if -10 < mean1 < 20 and -10 < mean2 < 20 and 0 < std < 10: \n",
    "        return 0\n",
    "    return -np.inf\n",
    "\n",
    "def propose_step(theta, scale): \n",
    "    \"\"\"\n",
    "    Propose new step: perturb the previous step\n",
    "    by adding random-normal values to the previous step\n",
    "    \"\"\"\n",
    "    return theta + scale * np.random.randn(len(theta))\n",
    "\n",
    "def simulate_dataset(theta): \n",
    "    \"\"\"\n",
    "    Simulate a dataset by generating a bimodal distribution\n",
    "    with means mu_1, mu_2 and standard deviation sigma\n",
    "    \"\"\"\n",
    "    mean1, mean2, std = theta\n",
    "    return np.concatenate([mean1 + std * np.random.randn(len(x)), \n",
    "                           mean2 + std * np.random.randn(len(x))])\n",
    "\n",
    "def summary_stats(y_sim):\n",
    "    \"\"\"\n",
    "    Compute the Anderson-Darling statistic as the distance\n",
    "    metric between the simulated observations y_sim and the \n",
    "    observations y_obs\n",
    "    \"\"\"\n",
    "    distance = anderson_ksamp([y_sim, y_obs]).statistic\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_sampler(theta, h, n_steps, scale=0.1, quiet=False):\n",
    "    \"\"\"\n",
    "    Follow algorithm written above for a simple rejection sampler. \n",
    "    \"\"\"\n",
    "    # Some bookkeeping variables:\n",
    "    accepted_steps = 0\n",
    "    total_steps = 0\n",
    "    samples = np.zeros((n_steps, len(theta)))\n",
    "    printed = set()\n",
    "\n",
    "    while accepted_steps < n_steps: \n",
    "\n",
    "        # Make a simple \"progress bar\":\n",
    "        if not quiet:\n",
    "            if accepted_steps % 1000 == 0 and accepted_steps not in printed:\n",
    "                printed.add(accepted_steps)\n",
    "                print(f'Sample {accepted_steps} of {n_steps}')\n",
    "\n",
    "        # Propose a new step:\n",
    "        new_theta = propose_step(theta, scale)\n",
    "        prior = lnprior(new_theta)\n",
    "\n",
    "        # If proposed step is within prior: \n",
    "        if np.isfinite(prior): \n",
    "\n",
    "            # Generate a simulated dataset from new parameters\n",
    "            y_sim = simulate_dataset(new_theta)\n",
    "\n",
    "            # Compute distance between simulated dataset\n",
    "            # and the observations\n",
    "            distance = summary_stats(y_sim)\n",
    "            total_steps += 1\n",
    "\n",
    "            # If distance is less than tolerance `h`, accept step:\n",
    "            if distance <= h: \n",
    "                theta = new_theta\n",
    "                samples[accepted_steps, :] = new_theta\n",
    "                accepted_steps += 1\n",
    "\n",
    "    print(f'Acceptance rate: {accepted_steps/total_steps}')\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run our rejection sampler for a given value of the tolerance $h$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial step parameters for the mean and std:\n",
    "theta = [true_mean1, true_mean2, true_std] \n",
    "\n",
    "# Number of posterior samples to compute\n",
    "n_steps = 5000\n",
    "\n",
    "\n",
    "# `h` is the distance metric threshold for acceptance;\n",
    "# try values of h between -0.5 and 5\n",
    "h = 0\n",
    "\n",
    "samples = rejection_sampler(theta, h, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`samples` now contains `n_steps` approximate posterior samples. Let's make a corner plot which shows the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corner import corner\n",
    "\n",
    "corner(samples, truths=[true_mean1, true_mean2, true_std], \n",
    "       levels=[0.6], labels=['mean 1', 'mean 2', 'std'], \n",
    "       show_titles=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with the above example by changing the values of from $h=-0.8$, for a more precise and more computationally expensive approximation to the posterior distribution, or to $h=10$ for a faster but less precise estimate of the posterior distribution. \n",
    "\n",
    "In practice, a significant fraction of your effort when applying ABC is spent balancing the computational expense of a small $h$ with the precision you need on your posterior approximation.\n",
    "\n",
    "We can see how the posterior distribution for the standard deviation $\\sigma$ varies as we vary $h$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_i = []\n",
    "h_range = [-0.5, 1, 5]\n",
    "\n",
    "for h_i in h_range: \n",
    "    samples_i.append(rejection_sampler(theta, h_i, n_steps, quiet=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_i, h_i in zip(samples_i, h_range): \n",
    "    plt.hist(s_i[:, 2], histtype='step', lw=2, \n",
    "             label=f\"h={h_i}\", density=True, \n",
    "             range=[0.5, 2], bins=50)\n",
    "plt.legend()\n",
    "plt.xlabel('std')\n",
    "plt.axvline(true_std, ls='--', color='DodgerBlue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, you can see that the posterior distribution for the standard deviation is largest for the largest $h$, and converges to a narrower distribution centered on the correct value as $h$ decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
