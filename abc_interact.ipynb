{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Demystifying Approximate Bayesian Computation\n",
    "\n",
    "#### Brett Morris\n",
    "\n",
    "### In this tutorial\n",
    "\n",
    "We will write our own rejection sampling algorithm to approximate the posterior distributions for some fitting parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import anderson_ksamp\n",
    "from corner import corner\n",
    "\n",
    "# The Anderson-Darling statistic often throws a harmless \n",
    "# UserWarning which we will ignore in this example \n",
    "# to avoid distractions:\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a set of observations \n",
    "\n",
    "First, let's generate a series of observations $y_\\mathrm{obs}$, taken at times $x$. The observations will be drawn from one of two Gaussian distributions with a fixed standard deviation, separated by $3\\sigma$ from one another. There will be a fraction $f$ of the total samples in the second mode of the distribution.\n",
    "\n",
    "In the plots that follow, blue represents the observations or the true input parameters, and shades of gray or black represent samples from the posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Standard deviation of both normal distributions\n",
    "true_std = 1\n",
    "\n",
    "# Mean of the first normal distribution\n",
    "true_mean1 = np.pi\n",
    "\n",
    "# Mean of the second normal distribution\n",
    "true_mean2 = 3 * np.pi\n",
    "\n",
    "# Fraction of samples in second mode: this \n",
    "# algorithm works best when the fraction \n",
    "# is between [0.2, 0.8]\n",
    "true_fraction = 0.3\n",
    "\n",
    "# Third number below is the number of samples to draw:\n",
    "x = np.linspace(0, 1, 500)\n",
    "\n",
    "# Generate a series of observations, drawn from \n",
    "# two normal distributions:\n",
    "y_obs = np.concatenate([true_mean1 + true_std * np.random.randn(int((1-true_fraction) * len(x))), \n",
    "                        true_mean2 + true_std * np.random.randn(int(true_fraction * len(x)))])\n",
    "\n",
    "# Plot the observations:\n",
    "plt.hist(y_obs, bins=50, density=True, color='#4682b4',\n",
    "         histtype='step', lw=3)\n",
    "plt.xlabel('$y_\\mathrm{obs}$', fontsize=20)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ax2.set_xticks([true_mean1, true_mean2])\n",
    "ax2.set_xticklabels(['$\\mu_1$', '$\\mu_2$'], fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does one fit for the means and standard deviations of the bimodal distribution? Since this example is a mixture of normal distirbutions, one way is to use [Gaussian mixture models](https://dfm.io/posts/mixture-models/), but we're going to take a different approach, which we'll see is more general later. \n",
    "\n",
    "## Approximate Bayesian Computation\n",
    "\n",
    "For this particular dataset, it's easy to construct a model $\\mathcal{M}$ which reproduces the observations $y_\\mathcal{obs}$ – the model is simply the concatenation of two normal distributions $\\mathcal{M} \\sim \\left[\\mathcal{N} \\left(\\mu_1, \\sigma, \\textrm{size=(1-f)N}\\right), \\mathcal{N}\\left(\\mu_2, \\sigma, \\textrm{size=}fN\\right)\\right]$, where the `size` argument determines the number of samples to draw from the distribution, $N$ is the total number of draws, and $f$ is the fraction of draws in the second mode. One way to *approximate* the posterior distributions of $\\theta = \\{\\mu_1, \\mu_2, \\sigma, f\\}$ would be to propose new parameters $\\theta^*$, and only keep a running list of the parameter combinations which produce a simulated dataset $y_\\mathrm{sim}$ which very closely reproduces the observations $y_\\mathrm{obs}$. \n",
    "\n",
    "\n",
    "*** \n",
    "\n",
    "### Summary statistic: the Anderson-Darling statistic\n",
    "\n",
    "In practice, this requires a *summary statistic*, which measures the \"distance\" between the simulated dataset $y_\\mathrm{sim}$ and the observations $y_\\mathrm{obs}$. In this example we need a metric which measures the probability that two randomly-drawn samples $y$ are drawn from the same distribution. One such metric is the [Anderson-Darling statistic](https://en.wikipedia.org/wiki/Anderson–Darling_test), which approaches a minimum near $A^2=-1.3$ for two sets $y$ that are drawn from indistinguishable distributions, and grows to $A^2 > 10^5$ for easily distinguishable distributions.\n",
    "\n",
    "We can see how the Anderson-Darling statistic behaves in this simple example below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "\n",
    "# Generate a bimodal distribution\n",
    "a = np.concatenate([np.random.randn(n_samples), \n",
    "                    3 + np.random.randn(n_samples//2)])\n",
    "\n",
    "# Plot the bimodal distribution\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3))\n",
    "ax[0].hist(a, color='silver', range=[-4, 11], bins=50,\n",
    "           lw=2, histtype='stepfilled')\n",
    "\n",
    "# For a set of bimodal distributions with varing means: \n",
    "for mean in [0, 1.2, 5]: \n",
    "    # Generate a new bimodal distribution\n",
    "    c = mean + np.concatenate([np.random.randn(n_samples), \n",
    "                               3 + np.random.randn(n_samples//2)])\n",
    "    \n",
    "    # Measure, plot the Anderson-darling statistic\n",
    "    a2 = anderson_ksamp([a, c]).statistic\n",
    "    ax[0].hist(c, histtype='step', range=[-4, 11], \n",
    "               bins=50, lw=2)\n",
    "\n",
    "    ax[1].plot(mean, a2, 'o')\n",
    "\n",
    "ax[0].set(xlabel='Samples', ylabel='Frequency')\n",
    "ax[1].set(xlabel='Mean', ylabel='$A^2$')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we have a set of observations $y_\\mathrm{obs}$ (left, gray) which we're comparing to the set of simulated observations $y_\\mathrm{sim}$ (left, colors). The Anderson-Darling statistic $A^2$ is plotted for each pair of the observations and the simulations (right). You can see that the minimum of $A^2$ is near -1.3, and it grows very large when $y_\\mathrm{obs}$ and $y_\\mathrm{sim}$ distributions are significantly different.\n",
    "\n",
    "In order to make our distance function approach zero when the Anderson-Darling statistic is at its minimum, we're going to rescale the outputs of the Anderson-Darling statistic a bit: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(y_obs, y_sim): \n",
    "    \"\"\"\n",
    "    Our distance metric between the observations y_obs\n",
    "    and the simulation y_sim will be the Anderson-Darling\n",
    "    Statistic A^2 + 1.31, so that its minimum value is \n",
    "    approximately 0 and its maximum value is >10^5.\n",
    "    \"\"\"\n",
    "    return anderson_ksamp([y_sim, y_obs]).statistic + 1.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### The rejection sampler\n",
    "\n",
    "We're now have the ingredients we need to create a *rejection sampler*, which will follow this algorithm: \n",
    "\n",
    "  1. Perturb initial/previous parameters $\\theta$ by a small amount to generate new trial parameters $\\theta^*$\n",
    "  \n",
    "  2. If the trial parameters $\\theta^*$ are drawn from within the prior, continue, else return to (1)\n",
    "  \n",
    "  3. Generate an example dataset $y_\\mathrm{sim}$ using your model $\\mathcal{M}$ \n",
    "  \n",
    "  4. Compute _distance_ between the simulated and observed datasets $\\rho(y_\\mathrm{obs}, y_\\mathrm{sim})$\n",
    "  \n",
    "  5. For some tolerance $h$, accept the step ($\\theta^* = \\theta$) if distance $\\rho(y_\\mathrm{obs}, y_\\mathrm{sim}) \\leq h$\n",
    "  \n",
    "  6. Return to step (1)\n",
    "  \n",
    "In the limit $h \\rightarrow 0$, the posterior samples are no longer an approximation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprior(theta): \n",
    "    \"\"\"\n",
    "    Define a prior probability, which simply requires \n",
    "    that -10 < mu_1, mu_2 < 20 and 0 < sigma < 10 and\n",
    "    0 < fraction < 1.\n",
    "    \"\"\"\n",
    "    mean1, mean2, std, fraction = theta\n",
    "    \n",
    "    if -10 < mean1 < 20 and -10 < mean2 < 20 and 0 < std < 10 and 0 <= fraction <= 1: \n",
    "        return 0\n",
    "    return -np.inf\n",
    "\n",
    "def propose_step(theta, scale): \n",
    "    \"\"\"\n",
    "    Propose new step: perturb the previous step\n",
    "    by adding random-normal values to the previous step\n",
    "    \"\"\"\n",
    "    return theta + scale * np.random.randn(len(theta))\n",
    "\n",
    "def simulate_dataset(theta): \n",
    "    \"\"\"\n",
    "    Simulate a dataset by generating a bimodal distribution\n",
    "    with means mu_1, mu_2 and standard deviation sigma\n",
    "    \"\"\"\n",
    "    mean1, mean2, std, fraction = theta\n",
    "    return np.concatenate([mean1 + std * np.random.randn(int((1-fraction) * len(x))), \n",
    "                           mean2 + std * np.random.randn(int(fraction * len(x)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_sampler(theta, h, n_steps, scale=0.1, quiet=False, \n",
    "                      y_obs=y_obs, prior=lnprior, \n",
    "                      simulate_y=simulate_dataset):\n",
    "    \"\"\"\n",
    "    Follow algorithm written above for a simple rejection sampler. \n",
    "    \"\"\"\n",
    "    # Some bookkeeping variables:\n",
    "    accepted_steps = 0\n",
    "    total_steps = 0\n",
    "    samples = np.zeros((n_steps, len(theta)))\n",
    "    printed = set()\n",
    "\n",
    "    while accepted_steps < n_steps: \n",
    "\n",
    "        # Make a simple \"progress bar\":\n",
    "        if not quiet:\n",
    "            if accepted_steps % 1000 == 0 and accepted_steps not in printed:\n",
    "                printed.add(accepted_steps)\n",
    "                print(f'Sample {accepted_steps} of {n_steps}')\n",
    "\n",
    "        # Propose a new step:\n",
    "        new_theta = propose_step(theta, scale)\n",
    "\n",
    "        # If proposed step is within prior: \n",
    "        if np.isfinite(prior(new_theta)): \n",
    "\n",
    "            # Generate a simulated dataset from new parameters\n",
    "            y_sim = simulate_y(new_theta)\n",
    "\n",
    "            # Compute distance between simulated dataset\n",
    "            # and the observations\n",
    "            dist = distance(y_obs, y_sim)\n",
    "            total_steps += 1\n",
    "\n",
    "            # If distance is less than tolerance `h`, accept step:\n",
    "            if dist <= h: \n",
    "                theta = new_theta\n",
    "                samples[accepted_steps, :] = new_theta\n",
    "                accepted_steps += 1\n",
    "\n",
    "    print(f'Acceptance rate: {accepted_steps/total_steps}')\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run our rejection sampler for a given value of the tolerance $h$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial step parameters for the mean and std:\n",
    "theta = [true_mean1, true_mean2, true_std, true_fraction] \n",
    "\n",
    "# Number of posterior samples to compute\n",
    "n_steps = 5000\n",
    "\n",
    "\n",
    "# `h` is the distance metric threshold for acceptance;\n",
    "# try values of h between -0.5 and 5\n",
    "h = 5\n",
    "\n",
    "samples = rejection_sampler(theta, h, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`samples` now contains `n_steps` approximate posterior samples. Let's make a corner plot which shows the results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['$\\mu_1$', '$\\mu_2$', '$\\sigma$', '$f$']\n",
    "truths = [true_mean1, true_mean2, true_std, true_fraction]\n",
    "\n",
    "corner(samples, truths=truths, \n",
    "       levels=[0.6], labels=labels, \n",
    "       show_titles=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with the above example by changing the values of from $h=2$, for a more precise and more computationally expensive approximation to the posterior distribution, or to $h=10$ for a faster but less precise estimate of the posterior distribution. \n",
    "\n",
    "In practice, a significant fraction of your effort when applying ABC is spent balancing the computational expense of a small $h$ with the precision you need on your posterior approximation.\n",
    "\n",
    "We can see how the posterior distribution for the standard deviation $\\sigma$ changes as we vary $h$, from a small value to a larger value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_i = []\n",
    "h_range = [3, 5, 8]\n",
    "\n",
    "for h_i in h_range: \n",
    "    samples_i.append(rejection_sampler(truths, h_i, n_steps, quiet=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "for s_i, h_i in zip(samples_i, h_range): \n",
    "    for j, axis in enumerate(ax): \n",
    "        axis.hist(s_i[len(s_i)//2:, j], histtype='step', lw=2, \n",
    "                  label=f\"h={h_i}\", density=True, \n",
    "                  bins=30)\n",
    "        axis.set_xlabel(labels[j]) \n",
    "        axis.axvline(truths[j], ls='--', color='#4682b4')\n",
    "ax[0].set_ylabel('Posterior PDF')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, blue histograms are for the smallest $h$, then orange, then blue. You can see that the posterior distribution for the standard deviation is largest for the largest $h$, and converges to a narrower distribution centered on the correct value as $h$ decreases.\n",
    "\n",
    "Now let's inspect how the simulated distributions look, generated using the posterior samples for our input parameters $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = dict(bins=25, range=[0, 12], \n",
    "             histtype='step', density=True)\n",
    "\n",
    "for i in np.random.randint(0, len(samples_i), size=50):\n",
    "    plt.hist(simulate_dataset(samples_i[0][i, :]), \n",
    "             alpha=0.3, color='silver', **props)\n",
    "\n",
    "plt.hist(y_obs, color='#4682b4', lw=3, **props)\n",
    "plt.xlabel('$y_\\mathrm{obs}, y_\\mathrm{sim}$', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue histogram is the set of observations $y_\\mathrm{obs}$. Shown in silver are various draws from the simulated distributions with the parameters $\\theta$ drawn randomly from the posterior distributions from the previous rejection sampling. You can see that the simulated (silver) histograms are \"non-rejectable approximations\" to the observations (blue).\n",
    "\n",
    "*** \n",
    "\n",
    "## A non-Gaussian example\n",
    "\n",
    "Now let's do an example where things are less Gaussian. Our data will be distributed with a _beta distribution_, according to \n",
    "\n",
    "$$f(x; a,b) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha - 1} (1 - x)^{\\beta - 1},$$\n",
    "\n",
    "where \n",
    "\n",
    "$$B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha - 1} (1 - t)^{\\beta - 1} dt$$\n",
    "\n",
    "This new distribution has positive parameters $\\theta = \\{\\alpha, \\beta\\}$ which we can use ABC to infer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import beta\n",
    "\n",
    "np.random.seed(2019)\n",
    "\n",
    "# The alpha and beta parameters are the tuning parameter\n",
    "# for beta distributions. \n",
    "true_a = 15\n",
    "true_b = 2\n",
    "\n",
    "y_obs_beta = beta(true_a, true_b, size=len(x));\n",
    "\n",
    "plt.hist(y_obs_beta, density=True, histtype='step', color='#4682b4', lw=3)\n",
    "plt.xlabel('$y_\\mathrm{obs}$', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll sample the logarithm of the $\\alpha$ and $\\beta$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprior_beta(theta):\n",
    "    lna, lnb = theta\n",
    "    if -100 < lna < 100 and -100 < lnb < 100:\n",
    "        return 0\n",
    "    return -np.inf\n",
    "    \n",
    "def propose_step_beta(theta, scale): \n",
    "    \"\"\"\n",
    "    Propose new step: perturb the previous step\n",
    "    by adding random-normal values to the previous step\n",
    "    \"\"\" \n",
    "    return theta + scale * np.random.randn(len(theta))\n",
    "\n",
    "def simulate_dataset_beta(theta): \n",
    "    \"\"\"\n",
    "    Simulate a dataset by generating a bimodal distribution\n",
    "    with means mu_1, mu_2 and standard deviation sigma\n",
    "    \"\"\"\n",
    "    a, b = np.exp(theta)\n",
    "    return beta(a, b, size=len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll keep the Anderson-Darling statistic as our summary statistic, which is non-parametric and agnostic about the distributions of the two samples it is comparing. We will swap in our new observations, prior, and simulation function, but nothing else changes in the rejection sampling algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `h` is the distance metric threshold for acceptance;\n",
    "# try values of h between 1 and 5\n",
    "h = 1\n",
    "\n",
    "samples = rejection_sampler([np.log(true_a), np.log(true_b)], h, 2 * n_steps, \n",
    "                            y_obs=y_obs_beta,\n",
    "                            prior=lnprior_beta,\n",
    "                            simulate_y=simulate_dataset_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_beta = [r'$\\ln\\alpha$', r'$\\ln\\beta$']\n",
    "truths_beta = [np.log(true_a), np.log(true_b)]\n",
    "\n",
    "corner(samples, labels=labels_beta, \n",
    "       truths=truths_beta,\n",
    "       levels=[0.6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how random draws from the posterior distributions for $\\alpha$ and $\\beta$ compare with the observations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = dict(bins=25, range=[0.5, 1], \n",
    "             histtype='step', density=True)\n",
    "\n",
    "for i in np.random.randint(0, len(samples), size=100): \n",
    "    lna, lnb = samples[i, :]\n",
    "    a = np.exp(lna)\n",
    "    b = np.exp(lnb)\n",
    "    plt.hist(beta(a, b, size=len(x)), \n",
    "             alpha=0.3, color='silver', **props)\n",
    "    \n",
    "plt.hist(y_obs_beta, color='#4682b4', lw=3, **props)\n",
    "plt.xlabel('$y_\\mathrm{obs}, y_\\mathrm{sim}$', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the blue histogram is the set of observations $y_\\mathrm{obs}$. Shown in silver are various draws from beta distributions with the parameters $\\alpha$ and $\\beta$ drawn randomly from the posterior distributions from the previous rejection sampling chain. You can see that the simulated (silver) histograms are \"non-rejectable approximations\" to the observations (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
